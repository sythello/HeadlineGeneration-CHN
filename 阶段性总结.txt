2017-7-4
目前的思路很简单，就是一个Seq2Seq的模型，本质上是一个AutoEncoder。
模型的输入是一个字符序列（1维向量），是新闻的正文。
首先将序列中的词进行embedding（这里对中文处理，用的是字向量，没有做分词），变成2维的 (word_cnt,  dim)。（dim是字向量的维度）
然后encoder是一层LSTM，取最后一步输出向量作为context vector（好像是这个名字？），是1维的 (dim)。之后decoder部分是一层LSTM，每一步输入都为context vector，取每一步输出，是2维的 (title_len, dim)。
最后加一层Dense层，把输出的“词向量”（dim维的向量，每个对应一个词）映射到vocab_size维，然后进行softmax，作为输出词的概率分布。这里的输出为 (title_len, vocab_size)。
Loss采用的是cross_entropy（negative log-likelihood），即对于真正标题中每个正确的词被取到的概率P，取-log，再平均，作为一个样本（一则新闻）的loss函数。

目前简单起见，正文部分仅截取前30个字，标题仅截取前10个字，并且输出标题也限定为10个字。而且一个经验性的处理是把输入、输出都倒过来，也就是正文输入序列反序输入；最后输出的标题也会是反序的，因此再反序一次，变为正序。这样据说效果比较好，实验结果似乎也确实如此。

具体实现使用Keras，目前LSTM用的最简单的，没有双向，也没有attention（暂时不会搞）。Loss函数的实现稍微遇到一点麻烦，因为这里一个样本的输出是2维的（每个词的预测概率分布），因此没法用原生的cross_entropy，需要自定义。需要注意Keras里面自定义Loss函数，其参数y_true和y_pred必须是相同的维度（2维对2维，3维对3维，等等）（暂不确定是否要求相同的shape）。因此需要对于数据中的标题部分做处理，不能直接用字的id作为标签，需要转化为one-hot编码，这样才能与模型的输出保持相同维度。具体实现如下，其实看起来很简单……只是当时不知道需要相同维度，坑了很久。

目前模型能正确运行，但是输出还不能看，基本都是同一个字，大多数是“国”字重复10遍。对于日本新闻可能是“日”字重复10遍，中国的可能是“中”字重复10遍。最好的一个输出某个俄罗斯的新闻，结果是“俄” * 9 + “国”。但这个仍然是不能看。

下一步的话，可以先尝试简单的改进，多堆几层LSTM。还不行的话，尝试加入双向LSTM。之后再去尝试attention。

2017-7-17
目前已经尝试了多堆几层的LSTM，以及双向LSTM，在Keras里实现都很简单，但是效果仍然不行。
按照老师的指导，尝试把encoder分为两阶段，第一阶段LSTM输入每个句子，输出该句子的向量表示；第二阶段LSTM输入每个句子的向量表示，输出文档表示，作为context vector，然后decode。但是这样依然效果不行。
（后来为了方便训练改成了GRU）
所以就尝试加入了attention，目前的attention机制是这样的：在decode阶段，每一步的输入仍是context vector，但得到GRU的隐层向量h之后，在原输入句子中进行attention，权重为内积。得到向量称为o。然后把o作为隐层向量输入给下一步（替代原本输入给下一步的h）。
但是效果仍然不佳。现在的想法是尝试修改这个attention模型。参考爱丁堡大学的wmt17机器翻译模型，似乎是在attention之后又加了很多层的全联接才得到最终的“隐层向量”o，并且使用了很多的Layer Normalisation。或许我这里也可以尝试一下。